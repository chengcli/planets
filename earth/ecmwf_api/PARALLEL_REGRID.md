# Parallel Regridding Performance Improvements

This document describes the parallel processing features added to the regrid module to improve performance for large datasets.

## Overview

The regrid module now supports shared-memory parallel execution using Python's multiprocessing library. Both vertical interpolation and horizontal regridding can be parallelized, resulting in significant performance improvements for large datasets.

## Performance Improvements

Based on benchmarks on a 4-core CPU:

### Vertical Interpolation
- Small datasets (10,000 columns): ~1.2x speedup
- Medium datasets (48,000 columns): ~1.3x speedup  
- Large datasets (160,000 columns): ~1.3x speedup

### Full Regridding Pipeline
- Small datasets: ~1.6x speedup
- Medium datasets: ~1.7x speedup

Performance scales with the number of available CPU cores and dataset size.

## Usage

### Basic Usage with Auto-Parallelization (Recommended)

The simplest way to use parallelization is to set `n_jobs=None`, which automatically determines the optimal number of workers based on your data size and available CPUs:

```python
from regrid import regrid_pressure_to_height, compute_height_grid

# Your data...
var_tpll = ...  # (T, P, Lat, Lon)
rho_tpll = ...
topo_ll = ...
# etc.

# Regrid with automatic parallelization
result = regrid_pressure_to_height(
    var_tpll, rho_tpll, topo_ll,
    plev, lats, lons,
    x1f, x2f, x3f,
    planet_grav, planet_radius,
    bounds_error=False,
    n_jobs=None  # Auto mode (recommended)
)
```

### Explicit Control

You can explicitly control the number of parallel workers:

```python
# Sequential execution (original behavior)
result = regrid_pressure_to_height(..., n_jobs=1)

# Use 2 workers
result = regrid_pressure_to_height(..., n_jobs=2)

# Use 4 workers
result = regrid_pressure_to_height(..., n_jobs=4)

# Use all available CPUs
result = regrid_pressure_to_height(..., n_jobs=-1)
```

### Vertical Interpolation Only

```python
from regrid import vertical_interp_to_z

# Parallel vertical interpolation
result = vertical_interp_to_z(
    z_col, v_col, z_out,
    bounds_error=False,
    n_jobs=None  # Auto mode
)
```

## API Reference

### `n_jobs` Parameter

All regridding functions now accept an optional `n_jobs` parameter:

- `n_jobs=None` (default): Automatic parallelization based on data size
  - Uses sequential execution for small datasets
  - Automatically determines optimal number of workers for larger datasets
  - **Recommended for most users**

- `n_jobs=1`: Sequential execution (no parallelization)
  - Maintains original behavior
  - Useful for debugging or when deterministic execution order is needed

- `n_jobs=2, 3, 4, ...`: Use exactly N parallel workers
  - Useful when you want explicit control over resource usage

- `n_jobs=-1`: Use all available CPU cores
  - Maximum parallelization
  - Best for dedicated processing tasks

## Functions with Parallel Support

The following functions support the `n_jobs` parameter:

### ECMWF Regrid Module (`earth/ecmwf_api/regrid.py`)

- `vertical_interp_to_z()`: Vertical interpolation with parallel column processing
- `regrid_pressure_to_height()`: Full regridding pipeline with parallel vertical and horizontal interpolation
- `regrid_multiple_variables()`: Regrid multiple variables in parallel (NEW!)

### Simple Regrid Module (`earth/regrid.py`)

- `vertical_interp_to_z()`: Vertical interpolation with parallel column processing
- `regrid_txyz_from_txyp()`: Full regridding pipeline with parallel vertical and horizontal interpolation

## Backward Compatibility

The parallel features are fully backward compatible:

- If `n_jobs` is not specified, it defaults to `None` (auto mode)
- For small datasets, auto mode uses sequential execution (no overhead)
- All existing code continues to work without modification
- Results are numerically identical regardless of parallelization level

## Implementation Details

### Parallelization Strategy

1. **Vertical Interpolation**: Parallelized over columns
   - Each column (t, lat, lon) is interpolated independently
   - Work is distributed across multiple processes using `multiprocessing.Pool`
   - Efficient for large numbers of columns

2. **Horizontal Regridding**: Parallelized over time-height slices
   - Each (time, height) slice is regridded independently
   - Work is distributed across multiple processes
   - Efficient for large numbers of time steps and height levels

### Auto-Parallelization Logic

The auto mode (`n_jobs=None`) uses the following heuristics:

- **Vertical interpolation**: Parallelizes if dataset has > 100 columns
- **Horizontal regridding**: Parallelizes if dataset has > 10 time-height slices
- **Worker count**: `min(cpu_count, max(1, n_items // threshold))`

These thresholds balance parallelization overhead with performance gains.

## Best Practices

1. **Use auto mode by default**: `n_jobs=None` provides good performance without tuning
2. **Pre-compute heights**: When regridding multiple variables, compute heights once with `compute_height_grid()`
3. **Profile your workflow**: Use `benchmark_parallel.py` to measure performance for your specific datasets
4. **Consider memory**: Each worker requires additional memory; reduce `n_jobs` if running out of memory
5. **Avoid nested parallelization**: If calling regrid functions in parallel loops, use `n_jobs=1`

## Examples

### Example 1: Simple Usage

```python
from regrid import regrid_pressure_to_height

# Regrid with automatic parallelization
result = regrid_pressure_to_height(
    var_tpll, rho_tpll, topo_ll,
    plev, lats, lons, x1f, x2f, x3f,
    planet_grav, planet_radius,
    n_jobs=None
)
```

### Example 2: Multiple Variables with Pre-computed Heights (Traditional Approach)

```python
from regrid import compute_height_grid, regrid_pressure_to_height

# Compute heights once
z_tpll = compute_height_grid(rho_tpll, topo_ll, plev, planet_grav)

# Regrid multiple variables in parallel
temp_result = regrid_pressure_to_height(
    temp_tpll, rho_tpll, topo_ll,
    plev, lats, lons, x1f, x2f, x3f,
    planet_grav, planet_radius,
    z_tpll=z_tpll,  # Pre-computed heights
    n_jobs=-1  # Use all CPUs
)

humid_result = regrid_pressure_to_height(
    humid_tpll, rho_tpll, topo_ll,
    plev, lats, lons, x1f, x2f, x3f,
    planet_grav, planet_radius,
    z_tpll=z_tpll,  # Reuse heights
    n_jobs=-1
)
```

### Example 3: Multiple Variables in One Call (Recommended!)

```python
from regrid import regrid_multiple_variables, compute_height_grid

# Prepare multiple variables to regrid
variables = {
    'temperature': temp_tpll,
    'humidity': humid_tpll,
    'pressure': press_tpll,
    'wind_u': u_tpll,
    'wind_v': v_tpll,
}

# Optional: Pre-compute heights once for all variables
z_tpll = compute_height_grid(rho_tpll, topo_ll, plev, planet_grav)

# Regrid all variables in parallel
results = regrid_multiple_variables(
    variables, rho_tpll, topo_ll,
    plev, lats, lons, x1f, x2f, x3f,
    planet_grav, planet_radius,
    z_tpll=z_tpll,  # Pre-computed heights (optional)
    n_jobs=-1       # Use all CPUs (prioritizes across variables first)
)

# Access results
temp_tzyx = results['temperature']
humid_tzyx = results['humidity']
# ... etc.
```

The `n_jobs` parameter intelligently distributes workers:
- **Prioritizes across variables**: When multiple variables exist, workers are allocated to process different variables in parallel first
- **Then within variables**: Any remaining workers are distributed to parallelize within each variable's interpolation

For many variables, this approach is significantly faster than processing them sequentially!

### Example 4: Conservative Resource Usage

```python
# Limit to 2 workers to avoid overloading the system
result = regrid_pressure_to_height(
    var_tpll, rho_tpll, topo_ll,
    plev, lats, lons, x1f, x2f, x3f,
    planet_grav, planet_radius,
    n_jobs=2
)
```

## Benchmarking

Run the included benchmark script to measure performance on your system:

```bash
cd earth/ecmwf_api
python benchmark_parallel.py
```

This will test various dataset sizes and report speedup factors.

## Troubleshooting

### "Performance is worse with parallelization"

- Your dataset may be too small to benefit from parallelization
- Try `n_jobs=1` for sequential execution
- The auto mode (`n_jobs=None`) should handle this automatically

### "Out of memory errors"

- Reduce the number of workers: `n_jobs=2` or `n_jobs=1`
- Each worker requires additional memory for data copies

### "Results differ slightly from sequential"

- Floating-point operations may occur in different orders
- Differences should be within numerical precision (< 1e-10)
- Use `n_jobs=1` if exact reproducibility is critical

## Technical Notes

- Uses Python's `multiprocessing.Pool` for process-based parallelism
- Avoids GIL limitations of threading
- Data is pickled for inter-process communication
- Memory overhead: O(n_jobs Ã— slice_size)
- No external dependencies beyond standard library

## Future Improvements

Potential enhancements for future versions:

- Support for distributed computing (Dask, MPI)
- GPU acceleration for interpolation
- Adaptive load balancing
- Progress reporting for long-running operations
- Chunked processing for very large datasets

## References

- Original issue: [Improve regrid performance](https://github.com/MIPlanets/planets/issues/XXX)
- Python multiprocessing: https://docs.python.org/3/library/multiprocessing.html
